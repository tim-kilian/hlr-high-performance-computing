Schema fuer Jacobi

1) Datenaufteilung der Matrix auf die einzelnen Prozesse:
die ersten N-1 Prozesse bekommen gleich viele Zeilen der Gesamtmatrix (24x24) als Teilmatrix
zugewiesen. Die jeweilige Anzahl an Zeilen Nz erhält man, indem man die Anzahl der Zeilen 
der Gesamtmatrix (24) ganzzahlig durch (numProc-1) teilt. Der letzte Prozess erhält den Rest. Die 
Anzahl der Zeilen der Teilmatrix des letzten Prozesses, erhält man mit dem Modulo Befehl: 
mod(24,int(24/(numProc-1))).
Bei der Verteilung muss außerdem Beachtet werden, dass die einzelnen Prozesse zur Berechnung 
des Abtaststerns jeweils die Randwerte von anderen Prozessen benötigen. Die Teilmatrizen 2 bis 
N-1 benötigen deshalb zusätzlich jeweils eine Zeile unten und oben. Die Teilmatrix 1 benötigt nur 
eine Zeile unterhalb, die Teilmatrix N nur eine Zeile oberhalb. 
Bei der Aufteilung mit MPI_SCATTERV werden die Teilmatrizen deshalb so aufgeteilt, dass sich sich 
jeweils in einer Zeile "überlappen". So liegen für den ersten Iterationsschritt auch bereits 
alle benötigten Randwerte vor.  

2) Parallelisierungsschema 
Nach der Aufteilung mit MPI_SCATTERV haben alle Teilmatrizen alle benötigten Randwerte (siehe 1)). 
Der erste Schritt in jedem Teilprozess und jedem Iterationsschritt 
ist deshalb die Berechnung des Abtaststerns für jedes Matrixelement (außer für die Randwerte). 
Danach müssen die Ergebnisse, die von anderen Prozessen widerum als Randwerte benötigt werden 
kommuniziert werden: 
Die Prozesse i=1 bis i=N-1 senden ihre zweitunterste Zeile (nicht die Zeile mit den Randwerten, sondern die
darüber!) an den darauffolgenden Prozess (i+1). Der Prozess i+1 ersetzt seine oberste Zeile mit der empfangenen. 
Die Prozesse i=2 bis i=N senden ihre zweitoberste Zeile an den vorherigen Prozess (i-1). Der Prozess 
i-1 erstetzt seine unterste Zeile mit der empfangenen. 
Für einen einzelnen Prozess i bedeutet das: er führt zuerst die Berechnung durch. Danach sendet er
seine zweitunterste Zeile an den nächsten Prozess und seine zweitoberste an den vorherigen Prozess. 
Anschließend empfängt er jeweils eine Zeile vom vorherigen und nachfolgenden Prozess und ersetzt seine 
erste und letzte Zeile durch die empfangenen Zeilen. Bei der ersten (letzten)
Teilmatrix ist zu beachten, dass diese keine neuen Randwerte für ihre erste (letzte) Zeile empfängt.
In diesen Zeilen gelten die konstanten Randbedingungen, die zu Beginn festgesetzt werden.  

3) Abbruchkriterium
bei fester Iterationszahl:
Dieses Abbruchkriterium ist einfach zu implementieren. Die Schleife zur Berechnung und Kommunikation
wird genauso oft durchgeführt, wie die Anzahl an Iterationen ist. 
bei Iteration nach Genauigkeit: 
jeder Prozess setzt nach jedem Iterationsschritt eine logical Variable auf 0 oder 1. 
1 bedeutet, dass das Abbruchkriterium in diesem Prozess erfüllt ist, 0 bedeutet dass es nicht erfüllt 
ist. Diese Variable wird an den Masterprozess versendet, der mit MPI_REDUCE alle logicals der
Teilprozesse empfängt und das Minimum berechnet. Ist das Minimum 0, ist das Abbruchkriterium noch 
nicht in jedem Teilprozess erfüllt und es muss weiter iteriert werden. Ist das Minimum 1, kann
die Iteration gestoppt werden. Falls dies der Fall ist, sendet der Master mit MPI_BROADCAST eine logical
Variable an alle Teilprozesse, sodass diese "wissen", dass die Iteration gestoppt werden muss.   


 
